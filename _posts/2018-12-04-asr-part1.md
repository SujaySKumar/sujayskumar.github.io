---
layout: post
title: Audio and Speech Signal Processing (ASR Part 1)
date: '2018-12-04T13:35:39.001162'
author: Sujay S Kumar
tags: 
---

Link to the start of ASR series: [Automatic Speech Recognition (ASR Part 0)]({% post_url 2018-12-04-asr-intro %})

Input to the ASR system will be an audio file/stream that encodes the speech. How this speech is stored, transmitted and encoded is explored in this post.

How humans produce and perceive speech is vastly different from how we intuitively try to understand speech. Speech consists of multiple deterministic signals interspersed within indeterministic boundaries.

### Human Ear
Human ear can perceive sounds with frequencies in the range $$20Hz$$ to $$20kHz$$. But this is not linear i.e we are more sensitive to lower frequencies than higher frequencies. For eg, we can perceive the difference between a signal at $$1kHz$$ and a signal at $$2kHz$$ better than $$19kHz$$ and $$20kHz$$. This range is dependent on the individual and age.
This is why we use a logarithmic filterbank (called Mel filterbank). In this filterbank, the filters become  wider and longer as the frequency increases.
Mel filterbank of length 40 is typical. i.e 40 filters. It is represented as a matrix where each row is a filter.

Broad overview of audio pre-processing to accommodate human ear limitations:

Conversion to frequency domain (Fourier transform - we use only magnitude and not phase) -> Mel filterbank filtering -> Logarithmic compression (Again because of human ears).
### Audio and Signals
When a sound is produced, it creates pressure differences in air (sound waves). These pressure differences are responsible for the to-fro movements of the magnet in the coil of the microphone and this in turn generates current/speech signals. These signals that are generated and transmitted over the wire can be visually demonstrated using a waveform. 

*How do we store these speech signals in a computer?* Let us assume that we are storing it in 32 bit. This means that this alternating current (in ampere) will be converted to a number (positive/negative). For 32 bits, the max positive number it can store is $$x$$ (let's say) and max negative number it can store is $$-x$$. So the current will be converted to a series of numbers that is represented as a 32 bit float. 

Usually speech signals are stored in 32 bit. It can also be stored in 16 bit, 64 bit etc. The visual representation of this can be seen in a waveform.

![img](/assets/asr/part1/waveform.png)

In the previous step, we have amplitude on $$y$$ axis and time on $$x$$ axis. Now we need to convert it into the frequency scale (called spectrogram). For this, we first take a small piece of time segment (called frame) from the previous step and convert it into a frequency representation. Here, on $$x$$ axis we get the series of frames that we've sampled. On $$y$$ axis, we get the distribution over all frequencies (Like a softmax at each frame step).

![img](/assets/asr/part1/spectrogram.png)

For eg, let us say that the frame we sampled has a $$sine$$ curve and that this $$sine$$ function has a frequency $$ f$$ (which is fixed). Therefore, for this frame there will be a single peak at the frequency $$ f$$ and 0 everywhere else. If the entire waveform is the same $$sine$$ graph (i.e every frame in the waveform has the same curve), in each frequency distribution, there will be the same single peak at $$ f$$. This will result in a horizontal line (just like a softmax).

![img](/assets/asr/part1/sine-spectrogram.png)

In this frequency distribution, the max frequency it can take is a specification. Phones usually do $$8kHz$$. ASRs like Kaldi and CMUSphinx need recordings at or above $$16kHz$$. Normally we record in $$44kHz$$ for music.

*Sample Numbers*:

While converting the audio signal from amplitude domain to the frequency domain, we spoke about taking small time segments called frames (called sampling-rate). If the audio that we have is a $$16kHz$$ audio, we have $$16 \times 10^{3}$$ numbers for each $$1 s$$. Usually, the window size for sampling is $$25 ms$$ with a $$10 ms$$ step. Hence, each frame size will be $$16 \times 10^{3} \times 25 \times 10^{-3} = 400$$. Hence, we will get a vector of size $$400$$ every $$10 ms$$.

### Overview of audio processing

- **Dithering**: Adding a very small amount of noise to the signal to prevent mathematical issues during feature computation (in particular, taking the logarithm of 0).
- **DC-Removal**: Removing any constant offset from the waveform.
- **Pre-emphasis Filter**: Applying a high pass filter to the signal prior to feature extraction to counteract that fact that typically the voiced speech at the lower frequencies has much high energy than the unvoiced speech at high frequencies.
- **Mel Filtering**: Binning and applying the filter on each frame.
- **Log of Mel Frequencies**: Taking log of the values from previous step.

At the end of these steps, we get vectors called as MFCC (Mel-frequency cepstral coefficients) vectors which are then used as the representation of the audio. We can think of these MFCC vectors as analogous to word embeddings in the NLP domain.
