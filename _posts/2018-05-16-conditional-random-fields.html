---
layout: post
title: Conditional Random Fields
date: '2018-05-16T07:44:00.000-07:00'
author: Sujay S Kumar
tags: 
modified_time: '2018-05-16T07:44:12.687-07:00'
blogger_id: tag:blogger.com,1999:blog-925060870564624560.post-9095579379828525248
blogger_orig_url: http://sujayskumar.blogspot.com/2018/05/conditional-random-fields.html
---

<div dir="ltr" style="text-align: left;" trbidi="on">Conditional Random Field (CRF) is the go-to algorithm for sequence labeling problems. Initial attempts at sequence labeling such as POS tagging and Named Entity Recognition were accomplished using the Hidden Markov Models. Although HMMs gave promising results for the same, they suffered from the same drawbacks as using a Naive Bayes models i.e conditional independence.<br /><br />Both Naive Bayes (and HMM in extension) try to fit a model that maximizes the joint probability distribution <i>P(X , Y)</i>. This is flawed on two levels. One, since <i>X</i> is already given, trying to fit a joint distribution over both <i>X</i> and <i>Y</i> is computationally expensive, wasteful and in the end, we do not require the type of distribution the input variable follow. We just need to know what distribution the target variable follows given an input variable. Second, since we are assuming that the input variables are conditionally independent, if there exists some correlation between the variables, the model assigns high probability when these two variables occur together instead of generalizing over the input distribution. Hence, we can see that any perturbance in the input distribution has a significant effect over the output. Ideally, since we only care about the distribution of the target variable <i>Y</i>, this should not be under consideration. We should evaluate <i>P(Y | X)</i> instead of <i>P(X , Y)</i>.<br /><br />In hindsight, this might seem trivial and as the obvious choice for any classification problem. In fact, majority of the classifiers in use today optimize over this metric. Nevertheless, it was an important breakthrough when it was considered decades ago. <br /><br />A Conditional Random Field (CRF) does exactly this. It evaluates <i>P(Y | X)</i>. The purpose of this post is to delve deeper into the mathematics of a CRF and why CRF is considered to be the starting point for most of the current classification algorithms.<br /><br />Consider a function <i>Q(.)</i> as a real valued function. According to Markovian distribution theorem,<br /><br /><img src="https://latex.codecogs.com/gif.latex?P(X,Y)&amp;space;\propto&amp;space;exp(\sum&amp;space;_{c\in&amp;space;C}Q(c,&amp;space;X,Y))" title="P(X,Y) \propto exp(\sum _{c\in C}Q(c, X,Y))" /><br /><br />Where <i>C</i> is a set of feature verticals (or cliques). This equation provides us with the un-normalized joint probability distribution over <i>X</i> and <i>Y</i>. Since we are interested in calculating the conditional probability,<br /><br /><img src="https://latex.codecogs.com/gif.latex?P(Y|X)=&amp;space;\frac{exp(\sum&amp;space;_{c\in&amp;space;C}Q(c,&amp;space;X,Y))}{\sum&amp;space;_{X}exp(\sum&amp;space;_{c\in&amp;space;C}Q(c,&amp;space;X,Y))}" title="P(Y|X)= \frac{exp(\sum _{c\in C}Q(c, X,Y))}{\sum _{X}exp(\sum _{c\in C}Q(c, X,Y))}" /><br /><br />This is similar to Gibbs Distribution (or Softmax depending on the <i>Q(.)</i> function)<br />The above equation can be re written as<br /><br /><img src="https://latex.codecogs.com/gif.latex?P(Y|X)=&amp;space;\frac{\prod&amp;space;_{C}\phi&amp;space;_{C}(X,&amp;space;Y)}{Z}" title="P(Y|X)= \frac{\prod _{C}\phi _{C}(X, Y)}{Z}" /><br /><br />Where <img src="https://latex.codecogs.com/gif.latex?\phi&amp;space;_{C}(X,&amp;space;Y)&amp;space;=&amp;space;exp(Q(C,X,Y))" title="\phi _{C}(X, Y) = exp(Q(C,X,Y))" /> and <i>Z</i> is called as the Partition Function.<br /><br /><img src="https://latex.codecogs.com/gif.latex?Z&amp;space;=&amp;space;\sum&amp;space;_{X}exp(\sum&amp;space;_{c\in&amp;space;C}Q(c,&amp;space;X,Y))" title="Z = \sum _{X}exp(\sum _{c\in C}Q(c, X,Y))" /><br /><br />For a CRF which is used for sequence labelling such as PoS or NER, the graph forms a linear chain and the probability equation can be written as<br /><br /><img src="https://latex.codecogs.com/gif.latex?P(Y_{1},Y_{2},...,Y_{M}|X)&amp;space;=&amp;space;\frac{1}{Z}\prod&amp;space;_{i}\phi&amp;space;(Y_{i},X)\phi^{1}(Y_{i-1},Y_{i},X)" title="P(Y_{1},Y_{2},...,Y_{M}|X) = \frac{1}{Z}\prod _{i}\phi (Y_{i},X)\phi^{1}(Y_{i-1},Y_{i},X)" /><br /><br />Where,<br /><br /><img src="https://latex.codecogs.com/gif.latex?\phi&amp;space;()&amp;space;=&amp;space;exp(\sum&amp;space;_{k}\theta&amp;space;_{k}s_{k}(y_{i},x_{i},i))" title="\phi () = exp(\sum _{k}\theta _{k}s_{k}(y_{i},x_{i},i))" /><br /><br /><img src="https://latex.codecogs.com/gif.latex?\phi^{1}&amp;space;()&amp;space;=&amp;space;exp(\sum&amp;space;_{j}\lambda_{j}t_{j}(y_{i-1},y_{i},x_{i},i))" title="\phi^{1} () = exp(\sum _{j}\lambda_{j}t_{j}(y_{i-1},y_{i},x_{i},i))" /><br />where <i>s<sub>k</sub></i> is the state feature function and <i>t<sub>j</sub></i> is the transition feature function (Similar to HMMs).<br /><br />CRF is a generalized method of calculating <i>P(Y | X)</i> and is not restricted to just evaluating sequence label probabilities. The flexibility of CRFs stems from the ability to choose any function as <i>Q(X , Y)</i>. In the following section, we can see how the logistic function can also be derived from CRF.<br /><br />Consider the feature function,<br /><br /><img src="https://latex.codecogs.com/gif.latex?\phi&amp;space;(X_{i},Y)&amp;space;=&amp;space;exp(w_{i}1(X_{i},Y))" title="\phi (X_{i},Y) = exp(w_{i}1(X_{i},Y))" /><br /><br />where <i>1</i> is an indicator function. It can also be denoted as&nbsp; <img src="https://latex.codecogs.com/gif.latex?\phi&amp;space;(X_{i},Y)&amp;space;=&amp;space;exp(w_{i}*&amp;space;AND(X_{i},Y))" title="\phi (X_{i},Y) = exp(w_{i}* AND(X_{i},Y))" /><br /><br />Calculating the probabilities,<br /><br /><img src="https://latex.codecogs.com/gif.latex?\phi&amp;space;(X_{i},Y=1)&amp;space;=&amp;space;exp(w_{i}X_{i})" title="\phi (X_{i},Y=1) = exp(w_{i}X_{i})" /><br /><br /><img src="https://latex.codecogs.com/gif.latex?\phi&amp;space;(X_{i},Y=0)&amp;space;=&amp;space;exp(0)=1" title="\phi (X_{i},Y=0) = exp(0)=1" /><br /><br />Therefore, substituting the above results in the CRF equation, we get,<br /><br /><img src="https://latex.codecogs.com/gif.latex?P(Y|X)&amp;space;=&amp;space;\frac{\phi&amp;space;(X_{i},Y=0)&amp;space;*&amp;space;\phi&amp;space;(X_{i},Y=1)}{\phi&amp;space;(X_{i},Y=0)&amp;space;+&amp;space;\phi&amp;space;(X_{i},Y=1)}&amp;space;=&amp;space;\frac{exp(\sum&amp;space;_{i}w_{i}X_{i})}{1+exp(\sum&amp;space;_{i}w_{i}X_{i})}" title="P(Y|X) = \frac{\phi (X_{i},Y=0) * \phi (X_{i},Y=1)}{\phi (X_{i},Y=0) + \phi (X_{i},Y=1)} = \frac{exp(\sum _{i}w_{i}X_{i})}{1+exp(\sum _{i}w_{i}X_{i})}" /><br /><br />The above equation is nothing but the logistic equation. Hence, we can see that CRF is a generalized form of any machine learning classification problem.</div>