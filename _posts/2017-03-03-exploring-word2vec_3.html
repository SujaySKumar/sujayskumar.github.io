---
layout: post
title: Exploring Word2Vec
date: '2017-03-03T08:25:00.000-08:00'
author: Sujay S Kumar
tags: 
modified_time: '2017-03-03T08:25:01.960-08:00'
thumbnail: https://2.bp.blogspot.com/-4bCMkA62ZmU/WLcAPOxR8cI/AAAAAAAAIwI/6AakzRa7HoIdTHgV1pmm_D-idqZ5hxwbwCLcB/s72-c/equation.png
blogger_id: tag:blogger.com,1999:blog-925060870564624560.post-2750000337812608207
blogger_orig_url: http://sujayskumar.blogspot.com/2017/03/exploring-word2vec_3.html
---

<div dir="ltr" style="text-align: left;" trbidi="on">In my previous blog post, I explored some of the early ways of word embeddings and their shortcomings. The purpose of this post is to explore one of the most widely used word representations in the natural language processing industry today.<br /><br />Word2Vec was created by a team of researchers led by Tomas Mikolov at Google. According to Wikipedia,<br /><blockquote class="tr_bq"><i><b>Word2vec</b> is a group of related models that are used to produce <a href="https://www.blogger.com/null" title="Word embedding">word embeddings</a>. These models are shallow, two-layer <a class="mw-redirect" href="https://www.blogger.com/null" title="Neural network">neural networks</a> that are trained to reconstruct linguistic contexts of words. Word2vec  takes as its input a large corpus of text and produces a <a href="https://www.blogger.com/null" title="Vector space">vector space</a>, typically of several hundred <a class="mw-redirect" href="https://www.blogger.com/null" title="Dimensions">dimensions</a>, with each unique word in the <a href="https://www.blogger.com/null" title="Corpus linguistics">corpus</a> being assigned a corresponding vector in the space. <a class="mw-redirect" href="https://www.blogger.com/null" title="Word vectors">Word vectors</a> are positioned in the vector space such that words that share common  contexts in the corpus are located in close proximity to one another in  the space.</i></blockquote>There are many resources on the internet providing a detailed mathematical explanation and derivations of Word2Vec and the reason why they perform so well. I have included links to some of them at the end of this post. The purpose of this post is to explore Word2Vec in a graphical manner in order to get an intuitive feel of how Word2Vec works, what the vectors mean and why they perform so well.<br /><br />Let us look at the equation proposed by Mikolov,<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-4bCMkA62ZmU/WLcAPOxR8cI/AAAAAAAAIwI/6AakzRa7HoIdTHgV1pmm_D-idqZ5hxwbwCLcB/s1600/equation.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="152" src="https://2.bp.blogspot.com/-4bCMkA62ZmU/WLcAPOxR8cI/AAAAAAAAIwI/6AakzRa7HoIdTHgV1pmm_D-idqZ5hxwbwCLcB/s640/equation.png" width="640" /></a></div><br />As we can see from the equation, it is nothing but a softmax equation.<br /><br /><br />Intuitively, we can visualize the model as follows. The main operation being performed here is that of dimensionality reduction. We have seen the usage of neural networks for dimensionality reduction in <i>auto-encoders</i>. In auto-encoders, we trick the neural network to learn compressed representations of vectors by providing the same vector as both input and target. In this way, the neural network learns to reproduce a vector from a compressed representation. Hence, we can replace the high dimension input vectors by their lower dimension hidden activations (Or weight matrices).<br /><br />Similarly, in Word2Vec we trick the "neural net" to learn smaller dimensions representations of the huge dimensioned one hot vector.<br /><br />But unlike in auto-encoders, we do not provide the same vector as both target and input vector. Although this can be used to reduce dimensions, we need to capture the syntactic and semantic meaning of every word. In order to accomplish that, we need to consider a window of words around the current word (context). How do we add this context to the representation of the current word? We make the neural net predict the current word given its context (CBOW) or to predict the context given the current word (Skip Gram).<br /><br />Once the neural net is trained to predict a word given it's context, we are going to strip off the hidden-output connections (just like auto-encoders) and use the trained weight matrix between input and hidden as vector representations (just like auto-encoders)<br /><br />Graphically, the model can be visualized as follows:<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-yjUf2gPE_4E/WLcAYtoeluI/AAAAAAAAIwM/ylH_M991FIwYdxjq9CpQ2kD9OeTUT2ItgCLcB/s1600/word2vec_arch.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="506" src="https://1.bp.blogspot.com/-yjUf2gPE_4E/WLcAYtoeluI/AAAAAAAAIwM/ylH_M991FIwYdxjq9CpQ2kD9OeTUT2ItgCLcB/s640/word2vec_arch.png" width="640" /></a></div><br /><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">V - Vocabulary Size</span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">H - Size of Word2Vec vector&nbsp;</span> <br /><br />There is a distinct difference between the above model and a normal feed forward neural network. The hidden layer in Word2Vec are linear neurons i.e there is no activation function applied on the hidden activations.<br /><br />Also, we can see that the dimensions of input layer and the output layer is equal to the vocabulary size. The hidden layer dimension is the size of the vector which represents a word (which is a tunable hyper-parameter).<br /><br />Now, let us take an example in order to understand this better and we'll go through each layer separately. <br /><blockquote class="tr_bq"><h4 style="text-align: left;"><i><span class="inline_editor_value"><span class="rendered_qtext">"The night is darkest just before the dawn rises"</span></span></i></h4></blockquote>From the above corpus we can see that the vocabulary size is 8.<br />For simplicity, let us make the following assumptions:<br /><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">Window size = 1</span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">Word2Vec dimension = 3</span><br /><br />The initial one-hot representations will be<br /><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">The&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [1,0,0,0,0,0,0,0]</span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">night&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [0,1,0,0,0,0,0,0]</span></span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">is&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [0,0,1,0,0,0,0,0]</span></span></span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">darkest&nbsp;&nbsp;&nbsp; - [0,0,0,1,0,0,0,0]</span></span></span></span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">just&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [0,0,0,0,1,0,0,0]</span></span></span></span></span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">before&nbsp;&nbsp;&nbsp;&nbsp; - [0,0,0,0,0,1,0,0]</span></span></span></span></span></span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">dawn</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [0,0,0,0,0,0,1,0]</span></span></span></span></span></span></span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">rises</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [0,0,0,0,0,0,0,1]</span>&nbsp;</span>&nbsp;</span>&nbsp;</span>&nbsp;</span>&nbsp;</span>&nbsp;</span>&nbsp;</span> <br /><br />Let us consider the following scenario, where the current word is <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">darkest</span> and the context word is <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">just</span>. Let us assume that this is a Continuous Bag Of Words (CBOW) model where we predict the current word given the context words.<br /><br /><b>Input Layer:</b><br /><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">Word: <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">just</span></span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">Vector: </span></span></span></span></span><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">[0,0,0,0,1,0,0,0]</span></span></span></span></span></span></span><br /><br /><b>Hidden Layer:</b><br /><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">Dimension = 3</span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>input-hidden</sub></span> is of dimension <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">8X3</span><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>hidden-output<sub></sub></sub></span> is of dimension <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">3X8</span><br /><br />Since the input is a one-hot vector, the hidden activation is just a lookup operation. That is, hidden activation just looks up the row corresponding to the word ID in <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>input-hidden</sub></span> and passes it on to the output layer. As an illustration,<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-PEbbeRo97F4/WLcAlo7zFlI/AAAAAAAAIwQ/LtxSJwJb-nA4K6WAMG3rekML_VXKtAWKgCLcB/s1600/matrix.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="150" src="https://1.bp.blogspot.com/-PEbbeRo97F4/WLcAlo7zFlI/AAAAAAAAIwQ/LtxSJwJb-nA4K6WAMG3rekML_VXKtAWKgCLcB/s640/matrix.png" width="640" /></a></div><br /><br /><br /><br /><div class="separator" style="clear: both; text-align: center;"><br /></div><div class="separator" style="clear: both; text-align: center;"></div><b>Output Layer:</b><br /><br /><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">Target word - <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">dar<span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">kest</span></span> </span></span></span></span></span></span><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">[0,0,0,1,0,0,0,0]</span></span></span></span></span><br /><br />Since the output layer is a <i>softmax</i> layer, it produces a probability distribution across the words. Thus, the categorical logloss is calculated (and since this is a softmax, the error is just the difference between the target vector and the output vector). This error is then back propagated to the hidden layer.<br /><br />As you might have noticed, this procedure gives us two trained parameters. The <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>input-hidden</sub></span> and <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>hidden-output</sub></span>. Usually, <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>hidden-output</sub></span> is discarded.<br /><br />In the Mikolov's equation we saw above, <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">V<sub>w</sub></span> is the <b><i>inner representation</i></b> of the word i.e from <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>input-hidden</sub></span> and <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">V<span style="font-family: &quot;arial&quot; , &quot;helvetica&quot; , sans-serif;"><sup>I</sup></span><sub>w</sub></span> is the <b><i>outer representation</i></b> of the word i.e from <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>hidden-output</sub></span>.<br /><br /><i>Inner representation</i> is nothing but the representation of the word when it is the current word and <i>outer representation</i> is when the word occurs in the window of another word.<br /><br />After the training process, the <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>input-hidden</sub></span> is just a lookup table, given the index, it returns the 3 bit <i>inner representation</i> of the word. Similarly, <span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">W<sub>hidden-output</sub></span> is a lookup table for the <i>outer representation</i> of the word.<br /><br /><a href="https://4.bp.blogspot.com/-c8hMWWJlLYE/WLcFZ3dTqpI/AAAAAAAAIwk/vbvq5k-x_VUhGq5C823je2Iqj634KCnTgCLcB/s1600/lookup%25282%2529.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="416" src="https://4.bp.blogspot.com/-c8hMWWJlLYE/WLcFZ3dTqpI/AAAAAAAAIwk/vbvq5k-x_VUhGq5C823je2Iqj634KCnTgCLcB/s640/lookup%25282%2529.png" width="640" /></a> <br />&nbsp; <br /><b>Why does this work?</b><br /><br />This method accomplishes dual task of reducing the dimensionality and adding semantic meaning to the word.<i> How?</i><br /><br />Let us take 2 pairs of words. <i>('Computers', 'ML')</i> and <i>('NFL', 'Kobe')</i>. If we look at the above pairs, we can see that <i>'Computers'</i> and <i>'ML'</i> occur in the same context in the corpus. Hence, the representations of the words <i>'Computers'</i> and <i>'ML'</i> will be very close to each other (cosine similarity). Whereas, there is no way <i>'NFL'</i> would be a part of a discussion about <i>'ML'</i> (Although there is a Kobe challenge on Kaggle, which we are assuming is absent from the corpus) and hence the similarity between them will be very low. Since the context will be similar to <i>'Computer'</i> and <i>'ML'</i> and <i>'NFL'</i> and <i>'Kobe'</i>, the neural net is forced to learn similar representations for each of these pairs.<br /><br />This also handles stemming on a certain level, as <i>'Computers'</i> and <i>'Computer'</i> occur in the same context and can be interchanged. This also has the ability to handle acronyms. Since <i>'ML'</i> and <i>'Machine'</i> and <i>'Learning'</i> occur in the same contexts, all 3 words will have similar representations.<br /><br />One important thing to note is that Word2Vec does <i>not</i> consider the positional variable of the context words i.e whether the word <i>'Learning'</i> comes before <i>'Machine'</i> or after is immaterial to the model. It does not learn a different vector for <i>'Learning'</i> when it occurs before <i>'Machine'</i> or after.<br /><br />Here are some of the resources where you can find the detailed mathematical derivation for Word2Vec:<br /><br /><a href="http://www.1-4-5.net/~dmm/ml/how_does_word2vec_work.pdf">http://www.1-4-5.net/~dmm/ml/how_does_word2vec_work.pdf</a><br /><a href="http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf">http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf</a><br /><a href="https://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf">https://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf</a> </div>